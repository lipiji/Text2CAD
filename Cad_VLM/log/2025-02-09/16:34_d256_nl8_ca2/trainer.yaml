cad_decoder:
  ca_level_start: 2
  cad_seq_len: 272
  cdim: 256
  dropout: 0.1
  num_heads: 8
  num_layers: 8
  tdim: 1024
debug: false
info: 'Experiment 1: Base Model Training'
text_encoder:
  adaptive_layer:
    dropout: 0.1
    in_dim: 1024
    num_heads: 8
    out_dim: 1024
  text_embedder:
    cache_dir: /data/share/Pretrained_Models/
    max_seq_len: 512
    model_name: bert_large_uncased
train:
  batch_size: 16
  checkpoint_interval: 10
  checkpoint_path: ./ckpt/
  curriculum_learning_epoch: 0
  log_dir: ./log/
  lr: 0.0001
  num_epochs: 150
  num_workers: 20
  prefetch_factor: 10
train_data:
  cad_seq_dir: /data1/pjli/data/Text2CAD/cad_seq
  max_seq_len: 512
  prompt_path: /data1/pjli/data/Text2CAD/text2cad_v1.0/text2cad_v1.0.csv
  split_filepath: /data1/pjli/data/Text2CAD/text2cad_v1.0/train_test_val.json
val:
  nucleus_prob: 0
  val_batch: 5
