cad_decoder:
  ca_level_start: 2
  cad_seq_len: 272
  cdim: 256
  dropout: 0.1
  num_heads: 8
  num_layers: 8
  tdim: 1024
debug: false
info: 'Experiment 1: Base Model Training'
text_encoder:
  adaptive_layer:
    dropout: 0.1
    in_dim: 1024
    num_heads: 8
    out_dim: 1024
  text_embedder:
    cache_dir: /data/share/Pretrained_Models/
    max_seq_len: 512
    model_name: bert_large_uncased
train:
  batch_size: 16
  checkpoint_interval: 10
  checkpoint_path: null
  curriculum_learning_epoch: 0
  log_dir: ./ckpt/
  lr: 0.0001
  num_epochs: 150
  num_workers: 30
  prefetch_factor: 10
train_data:
  cad_seq_dir: /data/pjli/workspace/gitcodes/Text2CAD/data
  max_seq_len: 512
  prompt_path: /data1/pjli/data/Text2CAD/text2cad_v1.1/text2cad_v1.1.csv
  split_filepath: /data/pjli/workspace/gitcodes/Text2CAD/data/train_val_test_split.json
val:
  nucleus_prob: 0
  val_batch: 5
